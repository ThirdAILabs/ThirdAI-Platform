job "on-prem-llm-generation" {
  datacenters = ["dc1"]

  type = "service"

  group "on-prem-generation" {
    count = 4

    scaling {
      enabled = true
      min = 1
      max = 30
      policy {
        cooldown = "1m"
        evaluation_interval = "10s"

        check "avg_cpu" {
          source = "nomad-apm"
          query = "avg_cpu-allocated"
          query_window = "3m"
          strategy "target-value" {
            target = 30
          }
        }

        check "request_count" {
          source = "nomad-apm"
          query = "request_count"
          query_window = "3m"
          strategy "target-value" {
            target = 3
          }
        }
      }
    }

    network {
      port "on-prem-generation-http" {
      }
    }

    service {
      name = "on-prem-generation"
      port = "on-prem-generation-http"
      provider = "nomad"

      {# This code adds a the prefix 'on-prem-llm' to all endpoints in the service. #}
      tags = [
        "traefik.enable=true",
        "traefik.http.routers.on-prem-generation-http.rule=PathPrefix(`/on-prem-llm`)",
        "traefik.http.routers.on-prem-generation-http.middlewares=on-prem-llm-stripprefix",
        "traefik.http.middlewares.on-prem-llm-stripprefix.stripprefix.prefixes=/on-prem-llm",
        "traefik.http.routers.on-prem-generation-http.priority=10"
      ]
    }

    task "backend" {
      driver = "docker"

      config {
        image = "ghcr.io/ggerganov/llama.cpp:server"
        image_pull_timeout = "15m"
        ports = ["on-prem-generation-http"]
        volumes = ["{{ share_dir }}/gen-ai-models:/gen-ai-models"]
        auth {
          username = "{{ docker_username }}"
          password = "{{ docker_password }}"
          server_address = "ghrc.io"
        }
        args = [
          "-m", "/gen-ai-models/Phi-3-mini-4k-instruct-q4.gguf",
          "--host", "0.0.0.0",
          "--port", "${NOMAD_PORT_on_prem_generation_http}"
        ]
      }

      resources {
        cores = 10
        memory = 4000
      }
    }
  }
}
