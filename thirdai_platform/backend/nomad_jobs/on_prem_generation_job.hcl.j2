job "on-prem-llm-generation" {
  datacenters = ["dc1"]

  type = "service"

  group "on-prem-generation" {
    count = "{{ num_allocations }}"

    network {
      port "on-prem-generation-http" {
      }
    }

    service {
      name = "on-prem-generation"
      port = "on-prem-generation-http"
      provider = "nomad"

      {# This code adds the prefix 'on-prem-llm' to all endpoints in this service. #}
      tags = [
        "traefik.enable=true",
        "traefik.http.routers.on-prem-generation-http.rule=PathPrefix(`/on-prem-llm`)",
        "traefik.http.routers.on-prem-generation-http.middlewares=on-prem-llm-stripprefix",
        "traefik.http.middlewares.on-prem-llm-stripprefix.stripprefix.prefixes=/on-prem-llm",
        "traefik.http.routers.on-prem-generation-http.priority=10"
      ]
    }

    task "backend" {
      driver = "docker"

      config {
        image = "ghcr.io/ggerganov/llama.cpp:server"
        image_pull_timeout = "15m"
        ports = ["on-prem-generation-http"]
        volumes = ["{{ mount_dir }}:/gen-ai-models"]
        auth {
          username = "{{ docker_username }}"
          password = "{{ docker_password }}"
          server_address = "ghrc.io"
        }
        args = [
          "-m", "/gen-ai-models/{{ model_name }}",
          "--host", "0.0.0.0",
          "--port", "${NOMAD_PORT_on_prem_generation_http}"
        ]
      }

      resources {
        cpu=20000
        memory = 4000
      }
    }
  }
}
