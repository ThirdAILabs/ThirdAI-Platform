job "telemetry" {
  datacenters = ["dc1"]
  type        = "service"
  {% set nomad_monitoring_dir = "/model_bazaar/nomad-monitoring" -%}
  group "telemetry" {
    count = 1
    {#
      If there are more than one node in the cluster, 
        then this constraint ensures that telemetry job always run on client node,

      The issue originates from ownerships of files created by victoriametric on nfs.
        if server starts the job, files created is having (root, nomad_nfs) as ownership and group ownership
        if client starts the job, file files created is having (root, nomad_nfs) as ownership and group ownership,

        so, different client is able to restart the victoriametric task but not the server.
      
      Incase multiple nodes are in the cluster,
        Assumption: There will be more client nodes than server.
        so constrain the job to always get scheduled on client node.
    #}
    {% if platform == "docker" and target_count != "1" -%}
    constraint {
      attribute = "${node.class}"
      value     = "web_ingress"
      operator = "!="
    }
    {%- endif %}

    network {
      port "vicky-http" {
        to = 8428
      }
      port "loki-http" {
        to = 3100
      }

      port "grafana-http" {
        to = 3000
      }
    }

    task "victoriametrics" {
      driver = "docker"

      service {
        name     = "vicky-web"
        provider = "nomad"
        port     = "vicky-http"
        tags = [
          "traefik.enable=true",
          "traefik.http.routers.vicky-http.rule=PathPrefix(`/victoriametric`)",
          "traefik.http.routers.vicky-http.priority=10"
        ]
      }

      config {
        image = "{{ registry }}/victoria-metrics:tags-v1.102.1-1-g76115c611f"
        auth {
            username = "{{ docker_username }}"
            password = "{{ docker_password }}"
            server_address = "{{ registry }}"
          }
        
        ports = ["vicky-http"]
        args = [
          "--storageDataPath={{ nomad_monitoring_dir }}/victoriametric",
          "--retentionPeriod=1d",
          "--httpListenAddr=:${NOMAD_PORT_vicky_http}",
          "--promscrape.config={{ nomad_monitoring_dir }}/node_discovery/prometheus.yaml"
        ]
        {% if platform == "local" -%}
        extra_hosts = ["host.docker.internal:host-gateway"]
        {%- endif %}
        volumes = [
          "{{ share_dir }}:/model_bazaar"
        ]
      }
      resources {
        cpu    = 256
        memory = 600
      }
    }

    task "loki" {
      driver = "docker"

      config {
        image = "{{ registry }}/loki:k221-47b9075"
        auth {
          username = "{{ docker_username }}"
          password = "{{ docker_password }}"
          server_address = "{{ registry }}"
        }
        group_add = ["4646"]
        args = [
          "-config.file",
          "${NOMAD_TASK_DIR}/loki.yaml",
        ]

        volumes = [
          "{{ share_dir }}:/model_bazaar"
        ]
        ports = ["loki-http"]
        
      }

      resources {
        cpu    = 500
        memory = 200
      }

      /*
        Date: 19/08/2024
        Cannot use "data            = file(abspath("./../configs/loki.yaml"))" because file function only works via CLI currently.
        https://github.com/hashicorp/nomad/issues/19648#issuecomment-1881052624
        */
      template {
        data            = <<EOF
auth_enabled: false
server:
  http_listen_port: 3100
  grpc_listen_port: 9096
common:
  instance_addr: 127.0.0.1
  path_prefix: {{ nomad_monitoring_dir }}/loki
  storage:
    filesystem:
      chunks_directory: {{ nomad_monitoring_dir }}/loki/chunks
      rules_directory: {{ nomad_monitoring_dir }}/loki/rules
  replication_factor: 1
  ring:
    kvstore:
      store: inmemory
frontend:
  max_outstanding_per_tenant: 2048
pattern_ingester:
  enabled: true
limits_config:
  max_global_streams_per_user: 0
  ingestion_rate_mb: 50000
  ingestion_burst_size_mb: 50000
  volume_enabled: true
query_range:
  results_cache:
    cache:
      embedded_cache:
        enabled: true
        max_size_mb: 100
schema_config:
  configs:
    - from: 2020-10-27
      store: tsdb
      object_store: filesystem
      schema: v13
      index:
        prefix: index_
        period: 24h
ruler:
  alertmanager_url: http://localhost:9093
analytics:
  reporting_enabled: false
ingester:
  wal:
    flush_on_shutdown: true
EOF
        destination     = "${NOMAD_TASK_DIR}/loki.yaml"
      }

      service {
        name = "loki-web"
        port = "loki-http"
        provider = "nomad"
        tags = [
          "traefik.enable=true",
          "traefik.http.routers.loki-http.rule=PathPrefix(`/loki`)",
          "traefik.http.routers.loki-http.priority=10"
        ]
      }
    }

    task "grafana" {
      lifecycle {
        hook = "poststart"
        sidecar = true
      }

      driver = "docker"

      env {
        GF_LOG_LEVEL          = "DEBUG"
        GF_LOG_MODE           = "console"
        GF_SECURITY_ADMIN_USER = "{{ admin_username }}"
        GF_SECURITY_ADMIN_EMAIL = "{{ admin_mail }}"
        GF_SECURITY_ADMIN_PASSWORD = "{{ admin_password }}"
        GF_DATABASE_URL = "{{ grafana_db_url }}"
        GF_SERVER_ROOT_URL = "%(protocol)s://%(domain)s:%(http_port)s/grafana/"
        GF_SERVER_SERVE_FROM_SUB_PATH = "true"
        GF_SERVER_HTTP_PORT   = "${NOMAD_PORT_http}"
        GF_PATHS_PROVISIONING = "/local/grafana/provisioning"
        GF_PATHS_DATA = "{{ nomad_monitoring_dir }}/grafana"
      }

      config {
        image = "{{ registry }}/grafana:main-ubuntu"
        auth {
          username = "{{ docker_username }}"
          password = "{{ docker_password }}"
          server_address = "{{ registry }}"
        }
        group_add = ["4646"]
        ports = ["grafana-http"]
        volumes = [
          "{{ share_dir }}:/model_bazaar"
        ]
      }

      service {
        name = "grafana"
        port = "grafana-http"
        provider = "nomad"
        tags = [
          "traefik.enable=true",
          "traefik.http.routers.grafana-http.rule=PathPrefix(`/grafana`)",
          "traefik.http.routers.grafana-http.priority=10"
        ]
      }

      resources {
        cpu    = 256
        memory = 300
      }

      template {
        data        = <<EOF
apiVersion: 1
datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    {% if platform == "local" -%}
    url: http://host.docker.internal:{% raw %}{{ range nomadService "vicky-web" }}{{ .Port }}{{ end }}{% endraw %}
    {%- else -%}
    {% raw %}{{ range nomadService "vicky-web" }}url: http://{{ .Address }}:{{ .Port }}{{ end }}{% endraw %}
    {%- endif %}
    isDefault: true
    editable: false
  - name: Loki
    type: loki
    access: proxy
    {% if platform == "local" -%}
    url: http://host.docker.internal:{% raw %}{{ range nomadService "loki-web" }}{{ .Port }}{{ end }}{% endraw %}
    {%- else -%}
    {% raw %}{{ range nomadService "loki-web" }}url: http://{{ .Address }}:{{ .Port }}{{ end }}{% endraw %}
    {%- endif %}
    editable: false
EOF
        destination = "/local/grafana/provisioning/datasources/datasources.yaml"
      }

      template {
        data        = <<EOF
apiVersion: 1
providers:
  - name: dashboards
    type: file
    disableDeletion: true
    updateIntervalSeconds: 10
    allowUiUpdates: true
    options:
      foldersFromFilesStructure: true
      path: {{ nomad_monitoring_dir }}/telemetry_dashboards
EOF
        destination = "/local/grafana/provisioning/dashboards/dashboards.yaml"
      }
    }
  }
}