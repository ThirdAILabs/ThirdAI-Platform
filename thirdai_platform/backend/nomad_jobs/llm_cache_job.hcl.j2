job "llm-cache" {
  datacenters = ["dc1"]

  type = "service"

  group "llm-cache" {
    count = 1

    network {
      port "llm-cache-http" {
        {% if platform == "docker" %}
          to = 80
        {% endif %}
      }
    }

    service {
      name = "llm-cache"
      port = "llm-cache-http"
      provider = "nomad"

      tags = [
        "traefik.enable=true",
        "traefik.http.routers.llm-cache-http.rule=PathPrefix(`/cache/`)",
        "traefik.http.routers.llm-cache-http.priority=10"
      ]
    }

    task "backend" {
      {% if platform == "local" %}
        driver = "raw_exec"
      {% elif platform == "docker" %}  
        driver = "docker"

      {% endif %}

      env {
        MODEL_BAZAAR_ENDPOINT = "{{ model_bazaar_endpoint }}"
        LICENSE_KEY = "{{ license_key }}"
        {% if platform == "docker" %}
        MODEL_BAZAAR_DIR = "/model_bazaar"
        {% elif platform == "local" %}
        MODEL_BAZAAR_DIR = "{{ share_dir }}"
        {% endif %}
      }

      config {
        {% if platform == "docker" %}  
          image = "{{ registry }}/{{ image_name }}:{{ tag }}"
          image_pull_timeout = "15m"
          ports = ["llm-cache-http"]
          auth {
            username = "{{ docker_username }}"
            password = "{{ docker_password }}"
            server_address = "{{ registry }}"
          }
          volumes = [
            "{{ share_dir }}:/model_bazaar"
          ]
          command = "python3"
          args    = ["-m", "uvicorn", "main:app", "--app-dir", "{{ app_dir }}", "--host", "0.0.0.0", "--port", "80"]
        {% elif platform == "local" %}
          command = "/bin/sh"
          args    = ["-c", "cd {{ thirdai_platform_dir }} && {{ python_path }} -m uvicorn main:app --app-dir {{ app_dir }} --host 0.0.0.0 --port ${NOMAD_PORT_llm_cache_http}"]
        {% endif %}
      }

      resources {
        cpu = 2400
        memory = 5000
      }
    }
  }
}