{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Optional\n",
    "import time\n",
    "import re\n",
    "import uuid\n",
    "\n",
    "BASE_URL = \"http://34.236.171.80\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def login(email: str, password: str):\n",
    "    \"\"\"\n",
    "    Login a user with the given email and password.\n",
    "    If successful, this function returns an access token.\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/api/user/email-login\"\n",
    "    response = requests.get(url, auth=HTTPBasicAuth(email, password))\n",
    "    result = response.json()\n",
    "    token = result.get(\"data\", {}).get(\"access_token\")\n",
    "    return token\n",
    "\n",
    "def list_models(\n",
    "    name: str, token: str, domain=None, username=None, type=None, sub_type=None, access_level=None\n",
    "):\n",
    "    \"\"\"\n",
    "    List models based on filters for authenticated users.\n",
    "    Returns:\n",
    "    - List of models that match the provided filters.\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/api/model/list\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    params = {\n",
    "        \"name\": name,\n",
    "        \"domain\": domain,\n",
    "        \"username\": username,\n",
    "        \"type\": type,\n",
    "        \"sub_type\": sub_type,\n",
    "        \"access_level\": access_level,\n",
    "    }\n",
    "    response = requests.get(\n",
    "        url, headers=headers, params={k: v for k, v in params.items() if v is not None}\n",
    "    )\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_retrieval_model(model_id: str, query: str, token: str):\n",
    "    \"\"\"\n",
    "    Retrieves top k most relevant references to the query from the deployed model.\n",
    "    Parameters:\n",
    "    - model_id: <username>/<modelname>\n",
    "    - query: The query to search for.\n",
    "    - token: Authorization token from login\n",
    "    \"\"\"\n",
    "    # Define the URL for querying the deployed model\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    query_url = f\"{BASE_URL}/{model_id}/predict\"\n",
    "    # Set up the query parameters\n",
    "    base_params = {\"query\": query, \"top_k\": 5}\n",
    "    ndb_params = {\"constraints\": {}}\n",
    "    # Make a POST request to query the model\n",
    "    response = requests.post(\n",
    "        query_url,\n",
    "        json={\"base_params\": base_params, \"ndb_params\": ndb_params},\n",
    "        headers=headers,\n",
    "    )\n",
    "    # Check if the query was successful; if not, raise an exception\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Query failed: {response.status_code}, {response.text}\")\n",
    "    return response.json()[\"data\"][\"references\"]\n",
    "\n",
    "def query_sentiment_model(model_id: str, query: str, token: str):\n",
    "    \"\"\"\n",
    "    Retrieves top k most relevant references to the query from the deployed model.\n",
    "    Parameters:\n",
    "    - model_id: <username>/<modelname>\n",
    "    - query: The query to search for.\n",
    "    - token: Authorization token from login\n",
    "    \"\"\"\n",
    "    # Define the URL for querying the deployed model\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    query_url = f\"{BASE_URL}/{model_id}/predict\"\n",
    "    # Set up the query parameters\n",
    "    base_params = {\"query\": query, \"top_k\": 5}\n",
    "    # Make a POST request to query the model\n",
    "    response = requests.post(\n",
    "        query_url,\n",
    "        json=base_params,\n",
    "        headers=headers,\n",
    "    )\n",
    "    # Check if the query was successful; if not, raise an exception\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Query failed: {response.status_code}, {response.text}\")\n",
    "    return response.json()[\"data\"][\"predicted_classes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_classifier_predict(model_id: str, query: str, token: str, top_k=1):\n",
    "    \"\"\"\n",
    "    Predicts the NER tags for a given query.\n",
    "    Parameters:\n",
    "    - model_id: model ID as returned by create_token_classifier. You can also find the model ID in the list returned by list_models.\n",
    "    - query: The passage to predict the NER tags for.\n",
    "    - token: Authorization token from login\n",
    "    - top_k: The number of tags predicted for each token.\n",
    "    Returns a dictionary in this format:\n",
    "    {\n",
    "        \"text\": \"The text that was passed in\",\n",
    "        \"predicted_tags\": [\n",
    "            [\"TOP_TAG_FOR_FIRST_TOKEN\", \"SCORE_FOR_TOP_TAG_FOR_FIRST_TOKEN\", ...],\n",
    "            [\"TOP_TAG_FOR_SECOND_TOKEN\", \"SCORE_FOR_TOP_TAG_FOR_SECOND_TOKEN\", ...],\n",
    "            ...\n",
    "            [\"TOP_TAG_FOR_LAST_TOKEN\", \"SCORE_FOR_TOP_TAG_FOR_LAST_TOKEN\", ...]\n",
    "        ]\n",
    "    }\n",
    "    The number of tags predicted for each token is specified in the top_k parameter.\n",
    "    \"\"\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    query_url = f\"{BASE_URL}/{model_id}/predict\"\n",
    "    base_params = {\"query\": query, \"top_k\": top_k}\n",
    "    response = requests.post(\n",
    "        query_url,\n",
    "        json=base_params,\n",
    "        headers=headers,\n",
    "    )\n",
    "    # Check if the query was successful; if not, raise an exception\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Query failed: {response.status_code}, {response.text}\")\n",
    "    return response.json()[\"data\"]\n",
    "\n",
    "def obfuscate_pii(token_classifier_model_id: str, text_chunks: List[str], auth_token: str):\n",
    "    \"\"\"\n",
    "    Obfuscates PII in the references using the NER model by replacing them with placeholders.\n",
    "    Parameters:\n",
    "    - token_classifier_model_id: model ID as returned by create_token_classifier. You can also find the model ID in the list returned by list_models.\n",
    "    - text_chunks: A list of strings containing the text to obfuscate.\n",
    "    - auth_token: Authorization token from login\n",
    "    Returns a tuple containing:\n",
    "    - A list of strings containing the obfuscated PII information.\n",
    "    - A dictionary containing the mapping of obfuscated tokens to original tokens.\n",
    "    \"\"\"\n",
    "    token_to_tag = {}\n",
    "    token_counts = {}\n",
    "    for text in text_chunks:\n",
    "        text = \" \".join(text.split())\n",
    "        predicted_tags = token_classifier_predict(auth_token, token_classifier_model_id, text)\n",
    "        predicted_tags = predicted_tags[\"predicted_tags\"]\n",
    "        for i, token in enumerate(text.split()):\n",
    "            tag = predicted_tags[i][0]\n",
    "            if tag != \"O\":\n",
    "                if token not in token_to_tag:\n",
    "                    tg = f\"<{tag}>\"\n",
    "                    token_to_tag[token] = tg\n",
    "    token_counts = {v: 0 for k, v in token_to_tag.items()}\n",
    "    inverse_map = {}\n",
    "    for k, v in token_to_tag.items():\n",
    "        new_tag = v[:-1] + f\"_{token_counts[v]}>\"\n",
    "        inverse_map[new_tag] = k\n",
    "        token_to_tag[k] = new_tag\n",
    "        token_counts[v] += 1\n",
    "    output_text = []\n",
    "    for text in text_chunks:\n",
    "        text = \" \".join(text.split())\n",
    "        redacted_text = [\n",
    "            word if word not in token_to_tag else token_to_tag[word]\n",
    "            for word in text.split()\n",
    "        ]\n",
    "        output_text.append(\" \".join(redacted_text))\n",
    "    return output_text, inverse_map\n",
    "\n",
    "def restore_pii(text: str, tag_to_token: Dict[str, str]):\n",
    "    \"\"\"\n",
    "    Restores the PII in the text by replacing the placeholders with the original tokens.\n",
    "    Parameters:\n",
    "    - text: A string containing the obfuscated PII information.\n",
    "    - tag_to_token: A dictionary containing the mapping of obfuscated tokens to original tokens.\n",
    "\n",
    "    Returns a string containing the restored PII information.\n",
    "    \"\"\"\n",
    "    restored_text = []\n",
    "    for word in text.split():\n",
    "        word = strip_non_alphanumeric(word)\n",
    "        if word in tag_to_token.keys():\n",
    "            restored_text.append(tag_to_token[word])\n",
    "        else:\n",
    "            restored_text.append(word)\n",
    "    return \" \".join(restored_text)\n",
    "\n",
    "\n",
    "def strip_non_alphanumeric(word):\n",
    "    pattern = r\"^[^a-zA-Z0-9_<>\\s]+|[^a-zA-Z0-9_<>\\s]+$\"\n",
    "    cleaned_string = re.sub(pattern, \"\", word)\n",
    "    return cleaned_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(model_id: str, user_input: str, token: str, session_id: str = None):\n",
    "    \"\"\"\n",
    "    Sends a chat request to the /chat endpoint.\n",
    "    Parameters:\n",
    "    - user_input: The message or query from the user.\n",
    "    - token: Authorization token from login.\n",
    "    - session_id: (Optional) Session ID for maintaining conversation context.\n",
    "    Returns:\n",
    "    - Response from the chat API.\n",
    "    \"\"\"\n",
    "    chat_url = f\"{BASE_URL}/{model_id}/chat\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    payload = {\n",
    "        \"user_input\": user_input,\n",
    "        \"session_id\": session_id\n",
    "    }\n",
    "    response = requests.post(chat_url, json=payload, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Chat request failed: {response.status_code}, {response.text}\")\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = login(\"admin@thirdai.com\", \"password\")\n",
    "\n",
    "results = query_retrieval_model(\"d4c6f0f2-6de1-4afb-b0c8-73adc2093094\", \"how to upgrade ios version\", token)\n",
    "\n",
    "# results is a list of dictioaries with references\n",
    "# results[0].keys()\n",
    "# dict_keys(['id', 'text', 'context', 'source', 'metadata', 'source_id', 'score'])\n",
    "\n",
    "token_tags = token_classifier_predict(\"1c86a014-f724-4c29-95e8-bac292adfba4\", \"my name is david and my phone number is 9728172948\", token)\n",
    "# {'query_text': 'my name is david and my phone number is 9728172948', 'tokens': ['my', 'name', 'is', 'david', 'and', 'my', 'phone', 'number', 'is', '9729991112'], 'predicted_tags': [['O'], ['O'], ['O'], ['NAME'], ['O'], ['O'], ['O'], ['O'], ['O'], ['PHONENUMBER']]} \n",
    "\n",
    "sentiment_result = query_sentiment_model(\"ac58ccaf-8031-433a-9ab5-db20af83b978\", \"I hate this call\", token)\n",
    "\n",
    "sentiment_map = {\"0\": \"negative\", \"1\": \"neutral\", \"2\": \"positive\"}\n",
    "sentiment = sentiment_map[sentiment_result[0][0]]\n",
    "score = sentiment_result[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = login(\"admin@thirdai.com\", \"password\")\n",
    "\n",
    "sentiment_map = {\"0\": \"negative\", \"1\": \"neutral\", \"2\": \"positive\"}\n",
    "\n",
    "session_id = str(uuid.uuid4())\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Enter your query: \")\n",
    "    token_tags_result = token_classifier_predict(\"1c86a014-f724-4c29-95e8-bac292adfba4\", user_input, token)\n",
    "    tokens = token_tags_result['tokens']\n",
    "    predicted_tags = token_tags_result['predicted_tags']\n",
    "    sensitive_tokens = [tokens[i] for i in range(len(tokens)) if predicted_tags[i][0] != \"O\"]\n",
    "    if sensitive_tokens:\n",
    "        print(\"Sensitive Tokens Detected: \", sensitive_tokens)\n",
    "    else:\n",
    "        print(\"No Sensitive Tokens Detected.\")\n",
    "    sentiment_result = query_sentiment_model(\"ac58ccaf-8031-433a-9ab5-db20af83b978\", user_input, token)\n",
    "    sentiment = sentiment_map[sentiment_result[0][0]]\n",
    "    score = sentiment_result[0][1]\n",
    "    print(f\"Sentiment: {sentiment} (score: {score})\")\n",
    "    retrieval_results = query_retrieval_model(\"d4c6f0f2-6de1-4afb-b0c8-73adc2093094\", user_input, token)\n",
    "    reference_text = \"\\n\".join([result['text'] for result in retrieval_results])\n",
    "    # print(\"Reference Text:\\n\", reference_text)\n",
    "    prompt = f\"\"\n",
    "    response = chat(\"f7212483-e610-4d0f-b7b7-353cf307ed06\", \"how to upgrade ios version\", token, session_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
