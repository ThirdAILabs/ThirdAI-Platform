{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # type: ignore\n",
    "from requests.auth import HTTPBasicAuth # type: ignore\n",
    "import json\n",
    "import os\n",
    "from typing import List, Optional\n",
    "ip_addr = \"localhost\"\n",
    "BASE_URL = f\"http://{ip_addr}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signup(username: str, email: str, password: str):\n",
    "    url = f\"{BASE_URL}/api/user/email-signup-basic\"\n",
    "    payload = {\n",
    "        \"username\": username,\n",
    "        \"email\": email,\n",
    "        \"password\": password\n",
    "    }\n",
    "    response = requests.post(url, json=payload)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def login(email: str, password: str):\n",
    "    url = f\"{BASE_URL}/api/user/email-login\"\n",
    "    response = requests.get(url, auth=HTTPBasicAuth(email, password))\n",
    "    result = response.json()\n",
    "    token = result.get(\"data\", {}).get(\"access_token\")\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_models(name, token, domain=None, username=None, type=None, sub_type=None, access_level=None):\n",
    "    \"\"\"\n",
    "    List models based on filters for authenticated users.\n",
    "\n",
    "    Returns:\n",
    "    - List of models that match the provided filters.\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/api/model/list\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    params = {\n",
    "        \"name\": name,\n",
    "        \"domain\": domain,\n",
    "        \"username\": username,\n",
    "        \"type\": type,\n",
    "        \"sub_type\": sub_type,\n",
    "        \"access_level\": access_level,\n",
    "    }\n",
    "    response = requests.get(url, headers=headers, params={k: v for k, v in params.items() if v is not None})\n",
    "    return response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_model(model_identifier, token):\n",
    "    \"\"\"\n",
    "    Delete a specified model., model_identifier is username/modelname\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/api/model/delete\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    params = {\"model_identifier\": model_identifier}\n",
    "    response = requests.post(url, headers=headers, params=params)\n",
    "    return response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retriever_model(\n",
    "    model_name: str,\n",
    "    token: str,\n",
    "    base_model_identifier: Optional[str] = None,\n",
    "    files: Optional[List[str]] = None,  # Local file paths\n",
    "    s3_urls: Optional[List[str]] = None,  # S3 URLs\n",
    "    nfs_paths: Optional[List[str]] = None,  # NFS paths\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates and trains an NDB retriever model with local, S3, and NFS files.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name: The name of the model.\n",
    "    - token: Authorization token.\n",
    "    - base_model_identifier: (Optional) The identifier of the base model to use.\n",
    "    - files: (Optional) List of local file paths.\n",
    "    - s3_urls: (Optional) List of S3 URLs for files.\n",
    "    - nfs_paths: (Optional) List of NFS paths for files.\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/api/train/ndb\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "\n",
    "    # Construct file information based on the different sources\n",
    "    file_info = {\n",
    "        \"unsupervised_files\": [],\n",
    "        \"supervised_files\": [],\n",
    "        \"test_files\": []\n",
    "    }\n",
    "\n",
    "    if files:\n",
    "        for file_path in files:\n",
    "            file_info[\"unsupervised_files\"].append({\n",
    "                \"path\": file_path,\n",
    "                \"location\": \"local\"\n",
    "            })\n",
    "\n",
    "    if s3_urls:\n",
    "        for s3_url in s3_urls:\n",
    "            file_info[\"unsupervised_files\"].append({\n",
    "                \"path\": s3_url,\n",
    "                \"location\": \"s3\"\n",
    "            })\n",
    "\n",
    "    if nfs_paths:\n",
    "        for nfs_path in nfs_paths:\n",
    "            file_info[\"unsupervised_files\"].append({\n",
    "                \"path\": nfs_path,\n",
    "                \"location\": \"nfs\"\n",
    "            })\n",
    "\n",
    "    # Prepare the files for upload\n",
    "    upload_files = []\n",
    "    if files:\n",
    "        for file_path in files:\n",
    "            if os.path.isfile(file_path):\n",
    "                upload_files.append(('files', open(file_path, 'rb')))\n",
    "                \n",
    "    \n",
    "    upload_files.append((\"file_info\", (None, json.dumps(file_info), \"application/json\")))\n",
    "\n",
    "    try:\n",
    "        # Send the POST request to the /ndb endpoint\n",
    "        response = requests.post(\n",
    "            url,\n",
    "            headers=headers,\n",
    "            params={\n",
    "                \"model_name\": model_name,\n",
    "                \"base_model_identifier\": base_model_identifier,\n",
    "            },\n",
    "            files=upload_files\n",
    "        )\n",
    "\n",
    "        # Check for the response\n",
    "        if response.status_code == 200:\n",
    "            print(\"Model training job submitted successfully.\")\n",
    "            print(response.json())\n",
    "            return response.json()[\"data\"][\"model_id\"]\n",
    "        else:\n",
    "            print(\"Failed to submit the model training job.\")\n",
    "            print(response.json())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ner_model(\n",
    "    model_name: str,\n",
    "    token: str,\n",
    "    base_model_identifier: Optional[str] = None,\n",
    "    files: Optional[List[str]] = None,  # Local file paths\n",
    "    model_options: Optional[dict] = None  # Additional model options for UDT training\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a UDT model with local, S3, and NFS files.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name: The name of the model.\n",
    "    - token: Authorization token.\n",
    "    - base_model_identifier: (Optional) The identifier of the base model to use.\n",
    "    - files: (Optional) List of local file paths.\n",
    "    - model_options: (Optional) Dictionary of additional model options.\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/api/train/udt\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "\n",
    "    # Construct file information based on the different sources\n",
    "    file_info = {\n",
    "        \"supervised_files\": [],\n",
    "        \"test_files\": []\n",
    "    }\n",
    "\n",
    "    if files:\n",
    "        for file_path in files:\n",
    "            file_info[\"supervised_files\"].append({\n",
    "                \"path\": file_path,\n",
    "                \"location\": \"local\"\n",
    "            })\n",
    "    \n",
    "    # Prepare the files for upload\n",
    "    upload_files = []\n",
    "    if files:\n",
    "        for file_path in files:\n",
    "            if os.path.isfile(file_path):\n",
    "                upload_files.append(('files', open(file_path, 'rb')))\n",
    "                \n",
    "    upload_files.append((\"file_info\", (None, json.dumps(file_info), \"application/json\")))\n",
    "    \n",
    "    if model_options:\n",
    "        upload_files.append(\n",
    "            (\"model_options\", (None, json.dumps(model_options), \"application/json\"))\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        # Send the POST request to the /udt endpoint\n",
    "        response = requests.post(\n",
    "            url,\n",
    "            headers=headers,\n",
    "            params = {\n",
    "                \"model_name\": model_name,\n",
    "                \"base_model_identifier\": base_model_identifier,\n",
    "            },\n",
    "            files=upload_files\n",
    "        )\n",
    "\n",
    "        # Check for the response\n",
    "        if response.status_code == 200:\n",
    "            print(\"Model training job submitted successfully.\")\n",
    "            print(response.json())\n",
    "            return response.json()[\"data\"][\"model_id\"]\n",
    "        else:\n",
    "            print(\"Failed to submit the model training job.\")\n",
    "            print(response.json())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to wait for training to complete\n",
    "import time\n",
    "\n",
    "\n",
    "def await_train(token, model_identifier):\n",
    "    # Define the URL for checking training status\n",
    "    status_url = f\"{BASE_URL}/api/train/status\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "\n",
    "    while True:\n",
    "        # Make a GET request to check the training status\n",
    "        response = requests.get(status_url, params={\"model_identifier\": model_identifier}, headers=headers)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Failed to get training status: {response.status_code}, {response.text}\")\n",
    "        \n",
    "        # Check the training status\n",
    "        status = response.json()[\"data\"][\"train_status\"]\n",
    "        \n",
    "        if status == \"complete\":\n",
    "            print(\"Training completed successfully.\")\n",
    "            break\n",
    "        elif status == \"failed\":\n",
    "            raise Exception(\"Training failed.\")\n",
    "        \n",
    "        print(\"Training in progress...\")\n",
    "        time.sleep(10)  # Wait for 10 seconds before checking again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to deploy the trained model\n",
    "def deploy_model(token, model_identifier):\n",
    "    # Define the URL for model deployment\n",
    "    deploy_url = f\"{BASE_URL}/api/deploy/run\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    params = {\"model_identifier\": model_identifier}\n",
    "\n",
    "    # Make a POST request to deploy the model\n",
    "    response = requests.post(deploy_url, headers=headers, params=params)\n",
    "\n",
    "    # Extract deployment ID from the response\n",
    "    content = response.json()\n",
    "    deployment_id = content[\"data\"][\"model_id\"]\n",
    "    print(f\"Model deployed successfully. Deployment ID: {deployment_id}\")\n",
    "    return deployment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to wait for deployment to complete\n",
    "def await_deploy(token, model_identifier):\n",
    "    # Define the URL for checking deployment status\n",
    "    status_url = f\"{BASE_URL}/api/deploy/status\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "\n",
    "    while True:\n",
    "        # Make a GET request to check the deployment status\n",
    "        response = requests.get(status_url, params={\"model_identifier\": model_identifier}, headers=headers)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Failed to get deployment status: {response.status_code}, {response.text}\")\n",
    "        \n",
    "        # Check the deployment status\n",
    "        status = response.json()[\"data\"][\"deploy_status\"]\n",
    "        \n",
    "        if status == \"complete\":\n",
    "            print(\"Deployment completed successfully.\")\n",
    "            break\n",
    "        elif status == \"failed\":\n",
    "            raise Exception(\"Deployment failed.\")\n",
    "        \n",
    "        print(\"Deployment in progress...\")\n",
    "        time.sleep(10)  # Wait for 10 seconds before checking again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndb_deployment_query(token, model_id, query):\n",
    "    # Define the URL for querying the deployed model\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    query_url = f\"{BASE_URL}/{model_id}/predict\"\n",
    "    \n",
    "    # Set up the query parameters\n",
    "    base_params = {\"query\": query, \"top_k\": 5}\n",
    "    ndb_params = {\"constraints\": {}}\n",
    "\n",
    "    # Make a POST request to query the model\n",
    "    response = requests.post(\n",
    "        query_url,\n",
    "        json={\"base_params\": base_params, \"ndb_params\": ndb_params},\n",
    "        headers=headers,\n",
    "    )\n",
    "\n",
    "    # Check if the query was successful; if not, raise an exception\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Query failed: {response.status_code}, {response.text}\")\n",
    "\n",
    "    print(f\"Query results: {response.json()}\")\n",
    "    \n",
    "    return response.json()[\"data\"][\"references\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndb_upvote_reference(model_id: str, token, query: str, reference_id: str):\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    query_url = f\"{BASE_URL}/{model_id}/upvote\"\n",
    "    \n",
    "    # Set up the query parameters\n",
    "    text_id_pairs = [{\"query_text\": query, \"reference_id\": reference_id}]\n",
    "\n",
    "    # Make a POST request to query the model\n",
    "    response = requests.post(\n",
    "        query_url,\n",
    "        json={\"text_id_pairs\": text_id_pairs},\n",
    "        headers=headers,\n",
    "    )\n",
    "    \n",
    "    print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndb_associate_keywords(model_id: str, token, source: str, target: str):\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    query_url = f\"{BASE_URL}/{model_id}/associate\"\n",
    "    \n",
    "    # Set up the query parameters\n",
    "    text_pairs = [{\"source\": source, \"target\": target}]\n",
    "\n",
    "    # Make a POST request to query the model\n",
    "    response = requests.post(\n",
    "        query_url,\n",
    "        json={\"text_pairs\": text_pairs},\n",
    "        headers=headers,\n",
    "    )\n",
    "    \n",
    "    print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_doc_dict(path: str, doc_type: str):\n",
    "    \"\"\"\n",
    "    Creates a document dictionary for different document types.\n",
    "\n",
    "    Parameters:\n",
    "    path (str): Path to the document file.\n",
    "    doc_type (str): Type of the document location.\n",
    "\n",
    "    Returns:\n",
    "    dict[str, str]: Dictionary containing document details.\n",
    "\n",
    "    Raises:\n",
    "    Exception: If the document type is not supported.\n",
    "    \"\"\"\n",
    "    _, ext = os.path.splitext(path)\n",
    "    if ext == \".pdf\":\n",
    "        return {\"document_type\": \"PDF\", \"path\": path, \"location\": doc_type}\n",
    "    if ext == \".csv\":\n",
    "        return {\"document_type\": \"CSV\", \"path\": path, \"location\": doc_type}\n",
    "    if ext == \".docx\":\n",
    "        return {\"document_type\": \"DOCX\", \"path\": path, \"location\": doc_type}\n",
    "\n",
    "    raise Exception(f\"Please add a map from {ext} to document dictionary.\")\n",
    "\n",
    "def ndb_insert_document(model_id: str, token, local_file: str):\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    query_url = f\"{BASE_URL}/{model_id}/insert\"\n",
    "    \n",
    "    files = [(\"files\", open(local_file, \"rb\"))]\n",
    "    documents = [create_doc_dict(local_file, \"local\")]\n",
    "    \n",
    "    files.append((\"documents\", (None, json.dumps(documents), \"application/json\")))\n",
    "    \n",
    "    response = requests.post(\n",
    "        query_url,\n",
    "        files=files,\n",
    "        headers=headers,\n",
    "    )\n",
    "    \n",
    "    print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndb_sources(model_id: str, token):\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    query_url = f\"{BASE_URL}/{model_id}/sources\"\n",
    "\n",
    "    # Make a POST request to query the model\n",
    "    response = requests.get(\n",
    "        query_url,\n",
    "        headers=headers,\n",
    "    )\n",
    "    \n",
    "    print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndb_delete_document(model_id: str, token, source_id):\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    query_url = f\"{BASE_URL}/{model_id}/delete\"\n",
    "\n",
    "    # Make a POST request to query the model\n",
    "    response = requests.post(\n",
    "        query_url,\n",
    "        json={\"source_ids\": [source_id]},\n",
    "        headers=headers,\n",
    "    )\n",
    "    \n",
    "    print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import websockets\n",
    "\n",
    "# WebSocket server URL\n",
    "WS_URL = f\"ws://{ip_addr}/llm-dispatch/generate\"\n",
    "\n",
    "async def generate_text_with_openai(query, model, api_key):\n",
    "    \"\"\"\n",
    "    Generate text using the OpenAI provider via the WebSocket endpoint.\n",
    "\n",
    "    Parameters:\n",
    "    - query: The input text to generate from.\n",
    "    - model: The model name to use for generation (e.g., 'gpt-3.5-turbo').\n",
    "    - api_key: Your OpenAI API key.\n",
    "\n",
    "    Returns:\n",
    "    - The full generated text.\n",
    "    \"\"\"\n",
    "    async with websockets.connect(WS_URL) as websocket:\n",
    "        # Construct the input message specifically for OpenAI\n",
    "        input_message = {\n",
    "            \"query\": query,\n",
    "            \"model\": model,\n",
    "            \"provider\": \"openai\",  # Specify OpenAI as the provider\n",
    "            \"key\": api_key         # OpenAI API key\n",
    "        }\n",
    "\n",
    "        # Send the input message as JSON\n",
    "        await websocket.send(json.dumps(input_message))\n",
    "\n",
    "        full_response = \"\"  # Initialize the full response\n",
    "\n",
    "        # Receive messages from the WebSocket\n",
    "        while True:\n",
    "            response = await websocket.recv()\n",
    "            response_data = json.loads(response)\n",
    "\n",
    "            # Check for error status\n",
    "            if response_data[\"status\"] == \"error\":\n",
    "                print(\"Error:\", response_data[\"detail\"])\n",
    "                return None\n",
    "\n",
    "            # Collect the generated content\n",
    "            full_response += response_data[\"content\"]\n",
    "\n",
    "            # Stop receiving when end_of_stream is True\n",
    "            if response_data.get(\"end_of_stream\"):\n",
    "                break\n",
    "\n",
    "        return full_response  # Return the full generated response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_model_predict(token, model_id, query):\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    query_url = f\"{BASE_URL}/{model_id}/predict\"\n",
    "    \n",
    "    base_params = {\"query\": query, \"top_k\": 1}\n",
    "    \n",
    "    response = requests.post(\n",
    "        query_url,\n",
    "        json=base_params,\n",
    "        headers=headers,\n",
    "    )\n",
    "\n",
    "    # Check if the query was successful; if not, raise an exception\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Query failed: {response.status_code}, {response.text}\")\n",
    "\n",
    "    print(f\"Query results: {response.json()}\")\n",
    "    return response.json()[\"data\"]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(token, ner_model_id, references):\n",
    "    token_to_tag = {}\n",
    "    token_counts = {}\n",
    "\n",
    "    for reference in references:\n",
    "        text = reference[\"text\"]\n",
    "        text = \" \".join(text.split())\n",
    "        predicted_tags = ner_model_predict(token, ner_model_id, text)\n",
    "        for i, token in enumerate(text.split()):\n",
    "            tag = predicted_tags[i]\n",
    "            if tag[0][0] != \"O\":\n",
    "                if token not in token_to_tag or token_to_tag[token][1] < tag[0][1]:\n",
    "                    tg = (f\"<{tag[0][0]}>\", tag[0][1])\n",
    "                    token_to_tag[token] = tg\n",
    "    token_counts = {v[0]:0 for k, v in token_to_tag.items()}\n",
    "    inverse_map = {}\n",
    "\n",
    "    for k, v in token_to_tag.items():\n",
    "        token_to_tag[k] = v[0]\n",
    "        new_tag = v[0][:-1] + f\"_{token_counts[v[0]]}>\"\n",
    "        inverse_map[new_tag] = k\n",
    "        token_to_tag[k] = new_tag\n",
    "        token_counts[v[0]] += 1\n",
    "\n",
    "    output_text = []\n",
    "    for reference in references:\n",
    "        text = reference[\"text\"]\n",
    "        text = \" \".join(text.split())\n",
    "        redacted_text = [word if word not in token_to_tag else token_to_tag[word] for word in text.split()]\n",
    "        output_text.append(\" \".join(redacted_text))\n",
    "\n",
    "    return \"\\n\\n\".join(output_text), inverse_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def strip_non_alphanumeric(word):\n",
    "    pattern = r'^[^a-zA-Z0-9_<>\\s]+|[^a-zA-Z0-9_<>\\s]+$'\n",
    "    cleaned_string = re.sub(pattern, '', word)\n",
    "    return cleaned_string\n",
    "def restore(text, tag_to_token):\n",
    "    restored_text = []\n",
    "    for word in text.split():\n",
    "        word = strip_non_alphanumeric(word)\n",
    "        if word in tag_to_token.keys():\n",
    "            restored_text.append(tag_to_token[word])\n",
    "        else:\n",
    "            restored_text.append(word)\n",
    "    return \" \".join(restored_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "\n",
    "token = login(email=\"yash@thirdai.com\", password=\"password\")\n",
    "\n",
    "ndb_model_id = create_retriever_model(\"ndb_model\", token=token, files=[\"/Users/yashwanthadunukota/ThirdAI-Platform/data/scifact/insert.pdf\"])\n",
    "\n",
    "class Token:\n",
    "    id_column: str = \"target\"\n",
    "    query_column: str = \"source\"\n",
    "\n",
    "    target_labels: list[str] = [\"PER\", \"ORG\"]\n",
    "    sub_type: str = \"token\"\n",
    "\n",
    "model_options = {\n",
    "    \"udt_options\": {\n",
    "        \"udt_sub_type\": Token.sub_type,\n",
    "        \"target_labels\": Token.target_labels,\n",
    "        \"source_column\": Token.query_column,\n",
    "        \"target_column\": Token.id_column,\n",
    "    }\n",
    "}\n",
    "token_model_id = create_ner_model(\"ner_model\", token=token, files=[\"/Users/yashwanthadunukota/ThirdAI-Platform/data/token/ner.csv\"], model_options=model_options)\n",
    "\n",
    "await_train(token=token, model_identifier=\"yash/ndb_model\")\n",
    "await_train(token=token, model_identifier=\"yash/ner_model\")\n",
    "\n",
    "deploy_model(token=token, model_identifier=\"yash/ndb_model\")\n",
    "deploy_model(token=token, model_identifier=\"yash/ner_model\")\n",
    "\n",
    "await_deploy(token=token, model_identifier=\"yash/ndb_model\")\n",
    "await_deploy(token=token, model_identifier=\"yash/ner_model\")\n",
    "\n",
    "\n",
    "async def generate_response(query, token, api_key=\"\"):\n",
    "    \n",
    "    references = ndb_deployment_query(token=token, model_id=ndb_model_id, query=query)\n",
    "    \n",
    "    context, token_to_tags = extract(token=token, ner_model_id=token_model_id, references=references)\n",
    "        \n",
    "    # Example usage\n",
    "    response = await generate_text_with_openai(\n",
    "        query=f\"query: {query}, answers: {references}, context: {context}\",\n",
    "        model=\"gpt-3.5-turbo\",  # Replace with the specific OpenAI model you want to use\n",
    "        api_key=api_key  # Replace with your actual OpenAI API key\n",
    "    )\n",
    "    \n",
    "    restored_text = restore(response, token_to_tags)\n",
    "    \n",
    "    print(restored_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await generate_response(query=\"alice in wonderland\",token=token, api_key=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
