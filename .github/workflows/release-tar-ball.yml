name: Release/Installation-testing Job

on:
  release:
    types: [published] # This triggers the workflow when a new release is published

  workflow_dispatch:
    inputs:
      installer_branch_name:
        description: "Ansible-repo branch name"
        required: true
        type: string
        default: "main"

jobs:
  build_docker_images:
    runs-on: ubuntu-latest-8-cores

    outputs:
      version: "v${{ steps.fetch_version.outputs.version }}"
      docker_image_branch_name: ${{ steps.fetch_branch_name.outputs.branch_name }}
      docker_registy_password: ${{ steps.fetch_branch_name.outputs.PULL_PASSWORD }}

    steps:
      - name: Checkout the repository
        uses: actions/checkout@v2

      - name: "Azure Login"
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      - name: Get the new version of the docker images
        id: fetch_version
        run: |
          if [[ "${{ github.event_name }}" == "release" && "${{ github.ref_name }}" == "main" ]]; then
            version=${{ github.event.release.tag_name }}
          else
            version=$(shuf -i 100-999 -n 3 | paste -sd '.')
          fi
          echo "Next version: $version"
          
          echo "version=$version" >> $GITHUB_OUTPUT
          echo "version=$version" >> $GITHUB_ENV

      - name: Set the branch name of the docker images
        id: fetch_branch_name
        run: |
          if [[ "${{ github.event_name }}" == "release" && "${{ github.ref_name }}" == "main" ]]; then
            branch_name="release-test-main"
          else
            branch_name=$(echo "test-${{ github.ref_name }}" | tr '_' '-')
          fi
          
          echo "Docker image branch to build on: $branch_name"
          echo "branch_name=$branch_name" >> $GITHUB_OUTPUT
          echo "branch_name=$branch_name" >> $GITHUB_ENV
      
      - name: Build and push docker images to the repository
        run: |
          cd release
          pip install -r requirements.txt
          python3 push.py -b ${{ env.branch_name }} --config config.yaml --no-cache --version ${{ env.version }}
      
      - name: Install yq for YAML parsing
        run: |
          sudo add-apt-repository ppa:rmescandon/yq
          sudo apt update
          sudo apt install yq -y

      - name: Extract the docker registry_password
        run: |
          PULL_PASSWORD=$(yq e '.azure.branches."'${branch_name}'".pull_credentials.password' $GITHUB_WORKSPACE/release/config.yaml)
          echo "PULL_PASSWORD=$PULL_PASSWORD" >> $GITHUB_OUTPUT
  
  package:
    runs-on: ubuntu-latest-8-cores
    needs: build_docker_images
    outputs:
      offline_package_name: ${{ steps.offline_pack.outputs.offline_tarfile_name }}
      online_package_name: ${{ steps.online_pack.outputs.online_tarfile_name }}
    steps:
      - name: Checkout the repository
        uses: actions/checkout@v2

      - name: Set up SSH for cloning
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.PLATFORM_INSTALLATION_DEPLOY_KEY }}" > ~/.ssh/id_ed25519
          chmod 600 ~/.ssh/id_ed25519
          ssh-keyscan github.com >> ~/.ssh/known_hosts

      - name: Clone Platform repository
        run: |
          git clone git@github.com:ThirdAILabs/platform.git
          cd platform
          git checkout ${{ github.event.inputs.installer_branch_name }}
          cd ..

      - name: Download and store the Llama model in the unified models folder
        run: |
          mkdir -p pretrained-models/genai
          wget -O pretrained-models/genai/Llama-3.2-1B-Instruct-f16.gguf https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-f16.gguf

      - name: Install Python and Dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y python3 python3-pip
          pip install transformers torch
      
      - name: Set HF_HOME environment variable
        run: echo "HF_HOME=$GITHUB_WORKSPACE/pretrained-models" >> $GITHUB_ENV

      - name: Run Model Download Script
        run: |
          python3 packaging/download_pretrained_transformers.py

      - name: Copy driver script to parent directory
        run: |
          cp ./packaging/driver.sh ./driver.sh

      - name: Copy config.yml to parent directory
        run: |
          cp ./platform/config.yml ./config.yml

      - name: Copy README.md to parent directory
        run: |
          cp ./packaging/README.md ./README.md

      - name: Upload platform package artifact
        uses: actions/upload-artifact@v3
        with:
          name: thirdai-platform-package
          path: thirdai-platform-package.tar.gz

      - name: Install AWS CLI
        run: |
          sudo apt-get update
          sudo apt-get install -y awscli
  
      - name: Configure AWS credentials
        run: |
          aws configure set aws_access_key_id ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws configure set aws_secret_access_key ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws configure set region us-east-1

      - name: Verify AWS CLI Setup
        run: |
          aws --version
          aws sts get-caller-identity

      - name: "Azure Login"
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      - name: Install Docker
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            apt-transport-https \
            ca-certificates \
            curl \
            software-properties-common
          curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
          sudo add-apt-repository \
            "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
          sudo apt-get update
          sudo apt-get install -y docker-ce

      - name: Download Docker images using the latest tag
        run: |
          # TODO(pratik): write a job to keep updating the images whose releases doesnot have versioning.
          mkdir -p docker_images-${{ needs.build_docker_images.outputs.version }}

          # Pull and save images using the persisted ${{ needs.build_docker_images.outputs.version }} across steps
          images=(thirdai_platform_${{ needs.build_docker_images.outputs.docker_image_branch_name }} frontend_${{ needs.build_docker_images.outputs.docker_image_branch_name }})

          for image in "${images[@]}"
          do
              echo "Pulling thirdaiplatform.azurecr.io/${image}:${{ needs.build_docker_images.outputs.version }}..."
              docker pull thirdaiplatform.azurecr.io/${image}:${{ needs.build_docker_images.outputs.version }}
              docker save -o docker_images-${{ needs.build_docker_images.outputs.version }}/${image}_${{ needs.build_docker_images.outputs.version }}.tar thirdaiplatform.azurecr.io/${image}:${{ needs.build_docker_images.outputs.version }}
          done

          docker pull thirdaiplatform.azurecr.io/nomad-autoscaler:0.3.7
          docker save -o docker_images-${{ needs.build_docker_images.outputs.version }}/nomad-autoscaler_0.3.7.tar thirdaiplatform.azurecr.io/nomad-autoscaler:0.3.7

          # TODO(pratik): Cleanup workflows/update_llama_cpp_docker.yml.
          docker pull thirdaiplatform.azurecr.io/llama.cpp:server
          docker save -o docker_images-${{ needs.build_docker_images.outputs.version }}/llama.cpp_server.tar thirdaiplatform.azurecr.io/llama.cpp:server

          docker pull thirdaiplatform.azurecr.io/keycloak:26.0.0
          docker save -o docker_images-${{ needs.build_docker_images.outputs.version }}/keycloak_26.0.0.tar thirdaiplatform.azurecr.io/keycloak:26.0.0

          docker pull registry:2
          docker save -o docker_images-${{ needs.build_docker_images.outputs.version }}/registry_2.tar registry:2

          docker pull postgres:latest
          docker save -o docker_images-${{ needs.build_docker_images.outputs.version }}/postgres_latest.tar postgres:latest

          docker pull thirdaiplatform.azurecr.io/traefik:v2.10
          docker save -o docker_images-${{ needs.build_docker_images.outputs.version }}/traefik_v2.10.tar thirdaiplatform.azurecr.io/traefik:v2.10

          docker pull thirdaiplatform.azurecr.io/grafana:main-ubuntu
          docker save -o docker_images-${{ needs.build_docker_images.outputs.version }}/grafana_main-ubuntu.tar thirdaiplatform.azurecr.io/grafana:main-ubuntu

          docker pull thirdaiplatform.azurecr.io/victoria-metrics:tags-v1.102.1-1-g76115c611f
          docker save -o docker_images-${{ needs.build_docker_images.outputs.version }}/victoria-metrics_tags-v1.102.1-1-g76115c611f.tar thirdaiplatform.azurecr.io/victoria-metrics:tags-v1.102.1-1-g76115c611f

          echo "All images pulled and saved in the docker_images-${{ needs.build_docker_images.outputs.version }} folder."

      - name: Package platform, model, driver script, config, and README into tarball
        id: online_pack
        run: |
          tarfile_name="thirdai-platform-package-${{ needs.build_docker_images.outputs.docker_image_branch_name }}-${{ needs.build_docker_images.outputs.version }}.tar.gz"
          echo "online_tarfile_name=$tarfile_name" >> $GITHUB_ENV
          echo "name=$tarfile_name" >> $GITHUB_OUTPUT
          tar -czvf $tarfile_name ./platform ./pretrained-models ./driver.sh ./config.yml ./README.md

      - name: Package Docker images, platform, model, driver script, config, and README into tarball
        id: offline_pack
        run: |
          tarfile_name="thirdai-platform-package-offline-${{ needs.build_docker_images.outputs.docker_image_branch_name }}-${{ needs.build_docker_images.outputs.version }}.tar.gz"
          echo "offline_tarfile_name=$tarfile_name" >> $GITHUB_ENV
          echo "name=$tarfile_name" >> $GITHUB_OUTPUT
          tar -czvf $tarfile_name ./platform ./docker_images-${{ needs.build_docker_images.outputs.version }} ./driver.sh ./config.yml ./README.md ./pretrained-models

      - name: Upload online platform package to S3
        run: |
          aws s3 cp ${{ env.online_tarfile_name }} s3://thirdai-corp-public/ThirdAI-Platform-latest-release/

      - name: Upload online platform package to S3
        run: |
          aws s3 cp ${{ env.offline_tarfile_name }} s3://thirdai-corp-public/ThirdAI-Platform-latest-release/

  setup_ec2_instances:
    runs-on: ubuntu-latest
    needs: [build_docker_images, package]

    outputs:
      server_machine_public_ip: ${{ steps.host_machine.outputs.public_ip }} 

    steps:
      - name: Install AWS CLI
        run: |
          sudo apt-get update
          sudo apt-get install -y awscli

      - name: Configure AWS credentials
        run: |
          aws configure set aws_access_key_id ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws configure set aws_secret_access_key ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws configure set region us-east-1

      - name: Verify AWS CLI Setup
        run: |
          aws --version
          aws sts get-caller-identity
      
      # These instances already have the aws configured to pull thirdai-packages from the s3 bucket
      - name: start the ec2-instance
        run: |
          aws ec2 start-instances --instance-ids i-08bfe31f7665561c5 i-00c741703fbf15a4d
      
      - name: Wait for EC2 to be running
        run: |
          echo "Waiting for EC2 instance to start..."
          sleep 30

      - name: Fetch the public ip of the server machine
        id: host_machine
        run: |
          PUBLIC_IP=$(aws ec2 describe-instances \
            --instance-ids i-08bfe31f7665561c5 \
            --query "Reservations[0].Instances[0].PublicIpAddress" \
            --output text)
          echo "public_ip=$PUBLIC_IP" >> $GITHUB_OUTPUT
          echo "public_ip=$PUBLIC_IP" >> $GITHUB_ENV

      - name: Set up SSH key
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.HOST_MACHINE_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
        
      - name: Add EC2 Host to Known Hosts
        run: ssh-keyscan -H ${{ secrets.HOST_MACHINE_PUBLIC_KEY }} >> ~/.ssh/known_hosts
      
      - name: Download the package and run driver.sh
        run: |
          ssh -o StrictHostKeyChecking=no ubuntu@$${{ env.public_ip }}  <<<EOF
            aws s3 cp s3://thirdai-corp-public/ThirdAI-Platform-latest-release/${{ needs.package.outputs.online_package_name }} .

            # unzip the package
            mkdir -p package
            tar -xvf ${{ needs.package.outputs.online_package_name }} -C package

            # Modify the config.yml
            yq -i '
              .license_path = "/home/ubuntu/ndb_enterprise_license.json" |
              .admin_mail = "${{ secrets.EMAIL }}" |
              .admin_username = "admin" |
              .admin_password = "${{ secrets.PASSWORD }}" |
              .genai_key = "${{ secrets.GENAI_KEY }}" |
              .thirdai_platform_version = "${{ needs.build_docker_images.outputs.version }}" |
              .platform_image_branch = "${{ needs.build_docker_images.outputs.docker_image_branch_name }}" |
              .docker_registry_password = "${{ needs.build_docker_images.outputs.docker_registy_password }}" |
              .login_method = "postgres" |
              .nodes[0].public_ip = "${{ env.public_ip }}" |
              .nodes[0].private_ip = "172.31.17.228" |
              .nodes[0].ssh_username = "ubuntu" |
              .nodes[0].connection_type = "local" |
              .nodes[1].private_ip = "172.31.31.123" |
              .nodes[1].ssh_username = "ubuntu" |
              .nodes[1].connection_type = "ssh" |
            '  ./package/config.yml

            # Run the driver.sh script
            source ./package/driver.sh ./package/config.yml -v 1>./package/installation.out 2>./package/installation.err
          EOF
      
      - name: Wait for 7 minutes to let the model_bazaar get started
        run: sleep $((7 * 60))

  test:
    strategy:
      fail-fast: false
    runs-on: ubuntu-latest
    needs: [build_docker_images, package, setup_ec2_instances]
    outputs:
      exit_code: ${{ steps.backend_test_status.outputs.exit_code }}

    steps:
      - name: Checkout the repository
        uses: actions/checkout@v2
      
      - name: Set up Python 3.9
        uses: actions/setup-python@v2
        with:
          python-version: 3.9

      - name: Install requirements
        run: |
          cd headless
          pip install -r requirements.txt

      - name: Env variables
        run: |
          echo "SHARE_DIR=$HOME/nfs/dir" >> $GITHUB_ENV
          echo "GENAI_KEY=${{ secrets.GENAI_KEY }}" >> $GITHUB_ENV
          mkdir -p ${{ env.SHARE_DIR }}

      - name: Generate random run name
        id: random-run-name
        run: |
          echo "random_run_name=test_$(python3 -c "import secrets; print(secrets.token_urlsafe(8))")" >> $GITHUB_ENV
      
      - name: Run tests
        id: backend_test_status
        run: |
          set -e
          commands=(
            python3 -m headless.run_locally --base-url "http://${{ needs.start_the_ec2_instances.outputs.server_machine_public_ip }}/api/" --email ${{ secrets.EMAIL }} --password ${{ secrets.PASSWORD }} --run-name ${{ env.random_run_name }} --dag NDB --on-prem --generation
            python3 -m headless.run_locally --base-url "http://${{ needs.start_the_ec2_instances.outputs.server_machine_public_ip }}/api/" --email ${{ secrets.EMAIL }} --password ${{ secrets.PASSWORD }} --run-name ${{ env.random_run_name }} --dag UDT
            python3 -m headless.run_locally --base-url "http://${{ needs.start_the_ec2_instances.outputs.server_machine_public_ip }}/api/" --email ${{ secrets.EMAIL }} --password ${{ secrets.PASSWORD }} --run-name ${{ env.random_run_name }} --dag UDT_DATAGEN
            python3 -m headless.run_locally --base-url "http://${{ needs.start_the_ec2_instances.outputs.server_machine_public_ip }}/api/" --email ${{ secrets.EMAIL }} --password ${{ secrets.PASSWORD }} --run-name ${{ env.random_run_name }} --dag Recovery_Backup
          )
          exit_code=0
          for cmd in "${commands[@]}"; do
            if ! eval "$cmd"; then
              exit_code=$?
              break
            fi
          done
          echo "exit_code=$exit_code" >> $GITHUB_OUTPUT

      - name: Test Next.js frontend
        if: always()
        id: frontend_test_status
        run: |
          BASE_URL_WITHOUT_API=$(echo "${{ needs.start_the_ec2_instances.outputs.server_machine_public_ip }}" | sed 's/\/api\/\?$//')
          echo "Testing frontend at $BASE_URL_WITHOUT_API"
          curl -f "$BASE_URL_WITHOUT_API" || exit 1
        
  cleanup:
    runs-on: ubuntu-latest
    needs: [build_docker_images, package, setup_ec2_instances, test]
    steps:
      - name: Install AWS CLI
        run: |
          sudo apt-get update
          sudo apt-get install -y awscli

      - name: Configure AWS credentials
        run: |
          aws configure set aws_access_key_id ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws configure set aws_secret_access_key ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws configure set region us-east-1

      - name: Verify AWS CLI Setup
        run: |
          aws --version
          aws sts get-caller-identity
      
      - name: run the cleanup script
        run: |
          ssh -o StrictHostKeyChecking=no ubuntu@${{ needs.setup_ec2_instances.outputs.server_machine_public_ip }} <<<EOF

            source ./package/driver.sh ./package/config.yml --cleanup -v 1>./package/cleanup.out 2>./package/cleanup.err
            rm -rf ./package "./${{ needs.package.outputs.online_package_name }}"
          EOF
      
      - name: Stop the ec2-machine
        run: |
          aws ec2 stop-instances --instance-ids i-08bfe31f7665561c5 i-00c741703fbf15a4d
      
      - name: Checkout code
        uses: actions/checkout@v2

      - name: Set up Python 3.9
        uses: actions/setup-python@v2
        with:
          python-version: 3.9
  
      # - name: Delete docker images
      #   if: github.event_name != 'release'
      #   run: |
      #     cd release
      #     pip install -r requirements.txt
      #     version="${{ needs.build_docker_images.outputs.version }}"
      #     echo "Deleting Docker images for branch ${{ needs.build_docker_images.outputs.docker_image_branch_name }} with version: $version"
      #     python3 docker_remove.py -b ${{ needs.build_docker_images.outputs.docker_image_branch_name }} --version "$version" --client_id ${{ secrets.APPLICATION_ID }} --tenant_id ${{ secrets.TENANT_ID }} --secret ${{ secrets.SECRET }} --config config.yaml
      
      # - name: Delete the offline and online tarballs from the s3 buckets
      #   run: |
      #     echo "Deleting package ${{ needs.package.outputs.offline_package_name }} and ${{ needs.package.outputs.online_package_name }} from S3..."
      #     aws s3 rm s3://thirdai-corp-public/ThirdAI-Platform-latest-release/${{ needs.package.outputs.offline_package_name }}
      #     aws s3 rm s3://thirdai-corp-public/ThirdAI-Platform-latest-release/${{ needs.package.outputs.online_package_name }}
        
