{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ThirdAI Platform Operations Cheat Sheet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.auth import HTTPBasicAuth  # type: ignore\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Optional\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "ip_addr = \"98.82.124.0\"\n",
    "BASE_URL = f\"http://{ip_addr}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signup(username: str, email: str, password: str):\n",
    "    \"\"\"\n",
    "    Signup a user with the given username, email, and password.\n",
    "    Once the request is sent, the user will receive an email with a link to verify their account.\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/api/user/email-signup-basic\"\n",
    "    payload = {\"username\": username, \"email\": email, \"password\": password}\n",
    "    response = requests.post(url, json=payload)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def login(email: str, password: str):\n",
    "    \"\"\"\n",
    "    Login a user with the given email and password.\n",
    "    If successful, this function returns an access token.\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/api/user/email-login\"\n",
    "    response = requests.get(url, auth=HTTPBasicAuth(email, password))\n",
    "    result = response.json()\n",
    "    token = result.get(\"data\", {}).get(\"access_token\")\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_models(\n",
    "    name: str, token: str, domain=None, username=None, type=None, sub_type=None, access_level=None\n",
    "):\n",
    "    \"\"\"\n",
    "    List models based on filters for authenticated users.\n",
    "\n",
    "    Returns:\n",
    "    - List of models that match the provided filters.\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/api/model/list\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    params = {\n",
    "        \"name\": name,\n",
    "        \"domain\": domain,\n",
    "        \"username\": username,\n",
    "        \"type\": type,\n",
    "        \"sub_type\": sub_type,\n",
    "        \"access_level\": access_level,\n",
    "    }\n",
    "    response = requests.get(\n",
    "        url, headers=headers, params={k: v for k, v in params.items() if v is not None}\n",
    "    )\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_model(model_identifier: str, token: str):\n",
    "    \"\"\"\n",
    "    Delete a specified model. model_identifier is username/modelname\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/api/model/delete\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    params = {\"model_identifier\": model_identifier}\n",
    "    response = requests.post(url, headers=headers, params=params)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retriever_model(\n",
    "    model_name: str,\n",
    "    token: str,\n",
    "    base_model_identifier: Optional[str] = None,\n",
    "    files: Optional[List[str]] = None,  # Local file paths\n",
    "    s3_urls: Optional[List[str]] = None,  # S3 URLs\n",
    "    nfs_paths: Optional[List[str]] = None,  # NFS paths\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates and trains an NDB retriever model with local, S3, and NFS files.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name: The name of the model.\n",
    "    - token: Authorization token from login.\n",
    "    - base_model_identifier: (Optional) The identifier of the base model to use.\n",
    "    - files: (Optional) List of local file paths.\n",
    "    - s3_urls: (Optional) List of S3 URLs for files.\n",
    "    - nfs_paths: (Optional) List of NFS paths for files.\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/api/train/ndb\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "\n",
    "    # Construct file information based on the different sources\n",
    "    file_info = {\"unsupervised_files\": [], \"supervised_files\": [], \"test_files\": []}\n",
    "\n",
    "    if files:\n",
    "        for file_path in files:\n",
    "            file_info[\"unsupervised_files\"].append(\n",
    "                {\"path\": file_path, \"location\": \"local\"}\n",
    "            )\n",
    "\n",
    "    if s3_urls:\n",
    "        for s3_url in s3_urls:\n",
    "            file_info[\"unsupervised_files\"].append({\"path\": s3_url, \"location\": \"s3\"})\n",
    "\n",
    "    if nfs_paths:\n",
    "        for nfs_path in nfs_paths:\n",
    "            file_info[\"unsupervised_files\"].append(\n",
    "                {\"path\": nfs_path, \"location\": \"nfs\"}\n",
    "            )\n",
    "\n",
    "    # Prepare the files for upload\n",
    "    upload_files = []\n",
    "    if files:\n",
    "        for file_path in files:\n",
    "            if os.path.isfile(file_path):\n",
    "                upload_files.append((\"files\", open(file_path, \"rb\")))\n",
    "\n",
    "    upload_files.append(\n",
    "        (\"file_info\", (None, json.dumps(file_info), \"application/json\"))\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Send the POST request to the /ndb endpoint\n",
    "        response = requests.post(\n",
    "            url,\n",
    "            headers=headers,\n",
    "            params={\n",
    "                \"model_name\": model_name,\n",
    "                \"base_model_identifier\": base_model_identifier,\n",
    "            },\n",
    "            files=upload_files,\n",
    "        )\n",
    "\n",
    "        # Check for the response\n",
    "        if response.status_code == 200:\n",
    "            print(\"Model training job submitted successfully.\")\n",
    "            print(response.json())\n",
    "            return response.json()[\"data\"][\"model_id\"]\n",
    "        else:\n",
    "            print(\"Failed to submit the model training job.\")\n",
    "            print(response.json())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_token_classifier(model_name: str, task_prompt: str, tags: List[dict], token: str, num_sentences: int = 10_000, num_samples_per_tag: Optional[int] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Trains a token classification model that can detect the provided tags.\n",
    "    It first generates training samples which contain tokens similar to the provided tag examples, then trains a model with this data.\n",
    "    \n",
    "    Parameters:\n",
    "    - model_name: The name of the new model.\n",
    "    - task_prompt: A prompt for the task that the model will perform, e.g. \"Detect PII\"\n",
    "    - tags: A list of classes that tokens can be tagged with, accompanied by a description and examples, e.g.\n",
    "        [\n",
    "            {\"name\": \"NAME\", \"examples\": [\"John Smith\", \"Anshumali Shrivastava\"], \"description\": \"A person's name\"},\n",
    "            {\"name\": \"PHONE_NUMBER\", \"examples\": [\"123-123-1234\", \"123 123 1234\", \"(123)-123-1234\"], \"description\": \"American phone number\"},\n",
    "        ]\n",
    "    - num_samples: The number of samples that will be generated for model training.\n",
    "    - token: Authorization token from login.\n",
    "    \"\"\"\n",
    "    if num_samples_per_tag is None:\n",
    "        num_samples_per_tag = max((num_sentences // len(tags)), 50)\n",
    "    \n",
    "    # Set up the headers with authorization\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {token}'\n",
    "    }\n",
    "\n",
    "    # Prepare the form data\n",
    "    form_data = {\n",
    "        'datagen_options': json.dumps({\n",
    "            'task_prompt': task_prompt,\n",
    "            'datagen_options': {\n",
    "                'sub_type': 'token',\n",
    "                'task_prompt': task_prompt, # This can also be a distinct prompt about the domain of the task.\n",
    "                'tags': tags,\n",
    "                'num_sentences_to_generate': num_sentences,\n",
    "                'num_samples_per_tag': num_samples_per_tag,\n",
    "            }\n",
    "        })\n",
    "    }\n",
    "\n",
    "    # Make the POST request\n",
    "    response = requests.post(\n",
    "        f'{BASE_URL}/api/train/nlp-datagen?model_name={model_name}',\n",
    "        data=form_data,\n",
    "        headers=headers\n",
    "    )\n",
    "    print(response.content)\n",
    "    # Check for success\n",
    "    response.raise_for_status()\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def await_train(model_identifier: str, token: str):\n",
    "    \"\"\"\n",
    "    Blocks until the model has finished training.\n",
    "    \n",
    "    Parameters:\n",
    "    - model_identifier: <username>/<modelname>\n",
    "    - token: Authorization token from login\n",
    "    \"\"\"\n",
    "    # Define the URL for checking training status\n",
    "    status_url = f\"{BASE_URL}/api/train/status\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "\n",
    "    while True:\n",
    "        # Make a GET request to check the training status\n",
    "        response = requests.get(\n",
    "            status_url, params={\"model_identifier\": model_identifier}, headers=headers\n",
    "        )\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(\n",
    "                f\"Failed to get training status: {response.status_code}, {response.text}\"\n",
    "            )\n",
    "\n",
    "        # Check the training status\n",
    "        status = response.json()[\"data\"][\"train_status\"]\n",
    "\n",
    "        if status == \"complete\":\n",
    "            print(\"Training completed successfully.\")\n",
    "            break\n",
    "        elif status == \"failed\":\n",
    "            raise Exception(\"Training failed.\")\n",
    "\n",
    "        print(\"Training in progress...\")\n",
    "        time.sleep(10)  # Wait for 10 seconds before checking again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_model(model_identifier: str, token: str):\n",
    "    \"\"\"\n",
    "    Allocates resources to serve the model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model_identifier: <username>/<modelname>\n",
    "    - token: Authorization token from login\n",
    "    \"\"\"\n",
    "    # Define the URL for model deployment\n",
    "    deploy_url = f\"{BASE_URL}/api/deploy/run\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    params = {\"model_identifier\": model_identifier}\n",
    "\n",
    "    # Make a POST request to deploy the model\n",
    "    response = requests.post(deploy_url, headers=headers, params=params)\n",
    "\n",
    "    # Extract deployment ID from the response\n",
    "    content = response.json()\n",
    "    deployment_id = content[\"data\"][\"model_id\"]\n",
    "    print(f\"Model deployed successfully. Deployment ID: {deployment_id}\")\n",
    "    return deployment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def await_deploy(model_identifier: str, token: str):\n",
    "    \"\"\"\n",
    "    Blocks until the model is deployed.\n",
    "    \n",
    "    Parameters:\n",
    "    - model_identifier: <username>/<modelname>\n",
    "    - token: Authorization token from login\n",
    "    \"\"\"\n",
    "    # Define the URL for checking deployment status\n",
    "    status_url = f\"{BASE_URL}/api/deploy/status\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "\n",
    "    while True:\n",
    "        # Make a GET request to check the deployment status\n",
    "        response = requests.get(\n",
    "            status_url, params={\"model_identifier\": model_identifier}, headers=headers\n",
    "        )\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(\n",
    "                f\"Failed to get deployment status: {response.status_code}, {response.text}\"\n",
    "            )\n",
    "\n",
    "        # Check the deployment status\n",
    "        status = response.json()[\"data\"][\"deploy_status\"]\n",
    "\n",
    "        if status == \"complete\":\n",
    "            print(\"Deployment completed successfully.\")\n",
    "            break\n",
    "        elif status == \"failed\":\n",
    "            raise Exception(\"Deployment failed.\")\n",
    "\n",
    "        print(\"Deployment in progress...\")\n",
    "        time.sleep(10)  # Wait for 10 seconds before checking again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_retrieval_model(model_id: str, query: str, token: str):\n",
    "    \"\"\"\n",
    "    Retrieves top k most relevant references to the query from the deployed model.\n",
    "\n",
    "    Parameters:\n",
    "    - model_id: <username>/<modelname>\n",
    "    - query: The query to search for.\n",
    "    - token: Authorization token from login\n",
    "    \"\"\"\n",
    "    # Define the URL for querying the deployed model\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    query_url = f\"{BASE_URL}/{model_id}/predict\"\n",
    "\n",
    "    # Set up the query parameters\n",
    "    base_params = {\"query\": query, \"top_k\": 5}\n",
    "    ndb_params = {\"constraints\": {}}\n",
    "\n",
    "    # Make a POST request to query the model\n",
    "    response = requests.post(\n",
    "        query_url,\n",
    "        json={\"base_params\": base_params, \"ndb_params\": ndb_params},\n",
    "        headers=headers,\n",
    "    )\n",
    "\n",
    "    # Check if the query was successful; if not, raise an exception\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Query failed: {response.status_code}, {response.text}\")\n",
    "\n",
    "    print(f\"Query results: {response.json()}\")\n",
    "\n",
    "    return response.json()[\"data\"][\"references\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upvote_reference(model_id: str, query: str, reference_id: str, token: str):\n",
    "    \"\"\"\n",
    "    Upvotes a reference for a given query.\n",
    "\n",
    "    Parameters:\n",
    "    - model_id: model ID as returned by create_retrieval_model. You can also find the model ID in the list returned by list_models.\n",
    "    - query: The query for which the reference is upvoted.\n",
    "    - reference_id: The ID of the reference to upvote.\n",
    "    - token: Authorization token from login\n",
    "    \"\"\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    query_url = f\"{BASE_URL}/{model_id}/upvote\"\n",
    "\n",
    "    # Set up the query parameters\n",
    "    text_id_pairs = [{\"query_text\": query, \"reference_id\": reference_id}]\n",
    "\n",
    "    # Make a POST request to query the model\n",
    "    response = requests.post(\n",
    "        query_url,\n",
    "        json={\"text_id_pairs\": text_id_pairs},\n",
    "        headers=headers,\n",
    "    )\n",
    "\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def associate_keyphrases(model_id: str, source: str, target: str, token: str):\n",
    "    \"\"\"\n",
    "    Associates two keyphrases.\n",
    "\n",
    "    Parameters:\n",
    "    - model_id: model ID as returned by create_retrieval_model. You can also find the model ID in the list returned by list_models.\n",
    "    - source: The source keyphrase.\n",
    "    - target: The target keyphrase.\n",
    "    - token: Authorization token from login\n",
    "    \"\"\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    query_url = f\"{BASE_URL}/{model_id}/associate\"\n",
    "\n",
    "    # Set up the query parameters\n",
    "    text_pairs = [{\"source\": source, \"target\": target}]\n",
    "\n",
    "    # Make a POST request to query the model\n",
    "    response = requests.post(\n",
    "        query_url,\n",
    "        json={\"text_pairs\": text_pairs},\n",
    "        headers=headers,\n",
    "    )\n",
    "\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_retrieval_model_documents(model_id: str, local_files: List[str], token: str):\n",
    "    \"\"\"\n",
    "    Inserts documents into an existing retrieval model.\n",
    "\n",
    "    Parameters:\n",
    "    - model_id: model ID as returned by create_retrieval_model. You can also find the model ID in the list returned by list_models.\n",
    "    - local_files: List of local file paths.\n",
    "    - token: Authorization token from login\n",
    "    \"\"\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    query_url = f\"{BASE_URL}/{model_id}/insert\"\n",
    "\n",
    "    files = [(\"files\", open(local_file, \"rb\")) for local_file in local_files]\n",
    "    documents = [create_doc_dict(local_file, \"local\") for local_file in local_files]\n",
    "\n",
    "    files.append((\"documents\", (None, json.dumps(documents), \"application/json\")))\n",
    "\n",
    "    response = requests.post(\n",
    "        query_url,\n",
    "        files=files,\n",
    "        headers=headers,\n",
    "    )\n",
    "\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def create_doc_dict(path: str, doc_type: str):\n",
    "    \"\"\"\n",
    "    Creates a document dictionary for different document types.\n",
    "\n",
    "    Parameters:\n",
    "    path (str): Path to the document file.\n",
    "    doc_type (str): Type of the document location.\n",
    "\n",
    "    Returns:\n",
    "    dict[str, str]: Dictionary containing document details.\n",
    "\n",
    "    Raises:\n",
    "    Exception: If the document type is not supported.\n",
    "    \"\"\"\n",
    "    _, ext = os.path.splitext(path)\n",
    "    if ext == \".pdf\":\n",
    "        return {\"document_type\": \"PDF\", \"path\": path, \"location\": doc_type}\n",
    "    if ext == \".csv\":\n",
    "        return {\"document_type\": \"CSV\", \"path\": path, \"location\": doc_type}\n",
    "    if ext == \".docx\":\n",
    "        return {\"document_type\": \"DOCX\", \"path\": path, \"location\": doc_type}\n",
    "\n",
    "    raise Exception(f\"Please add a map from {ext} to document dictionary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_retrieval_model_documents(model_id: str, token: str):\n",
    "    \"\"\"\n",
    "    Lists all documents that have been inserted into a retrieval model.\n",
    "\n",
    "    Parameters:\n",
    "    - model_id: model ID as returned by create_retrieval_model. You can also find the model ID in the list returned by list_models.\n",
    "    - token: Authorization token from login\n",
    "\n",
    "    Returns:\n",
    "    - A list of dictionaries containing the document details.\n",
    "    \"\"\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    query_url = f\"{BASE_URL}/{model_id}/sources\"\n",
    "\n",
    "    # Make a POST request to query the model\n",
    "    response = requests.get(\n",
    "        query_url,\n",
    "        headers=headers,\n",
    "    )\n",
    "\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_retrieval_model_document(model_id: str, source_id: str, token: str):\n",
    "    \"\"\"\n",
    "    Deletes a document from a retrieval model.\n",
    "\n",
    "    Parameters:\n",
    "    - model_id: model ID as returned by create_retrieval_model. You can also find the model ID in the list returned by list_models.\n",
    "    - source_id: The ID of the document to delete. You can find the source ID in the list returned by list_retrieval_model_documents.\n",
    "    - token: Authorization token from login\n",
    "    \"\"\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    query_url = f\"{BASE_URL}/{model_id}/delete\"\n",
    "\n",
    "    # Make a POST request to query the model\n",
    "    response = requests.post(\n",
    "        query_url,\n",
    "        json={\"source_ids\": [source_id]},\n",
    "        headers=headers,\n",
    "    )\n",
    "\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model_access_level(model_identifier: str, access_level: str, token: str):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - model_identifier: <owner username>/<model name>\n",
    "    - access_level: \"public\", \"protected\", \"private\"\n",
    "    - token: Authorization token from login\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing the model ID and the updated access level in this format:\n",
    "    {\n",
    "        \"model_id\": \"<model ID>\",\n",
    "        \"access_level\": \"<access level>\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    query_url = f\"{BASE_URL}/api/model/update-access-level\"\n",
    "    response = requests.post(\n",
    "        query_url,\n",
    "        params={\"model_identifier\": model_identifier, \"access_level\": access_level},\n",
    "        headers=headers,\n",
    "    )\n",
    "    \n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_model_access_to_user(model_identifier: str, email: str, permission: str, token: str):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - model_identifier: <owner username>/<model name>\n",
    "    - email: <user email>\n",
    "    - permission: \"read\", \"write\"\n",
    "    - token: Authorization token from login\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing the model ID and the updated access level in this format:\n",
    "    {\n",
    "        \"model_id\": \"<model ID>\",\n",
    "        \"user_id\": \"<user ID>\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    query_url = f\"{BASE_URL}/api/model/update-model-permission\"\n",
    "\n",
    "    response = requests.post(\n",
    "        query_url,\n",
    "        params={\"model_identifier\": model_identifier, \"email\": email, \"permission\": permission},\n",
    "        headers=headers,\n",
    "    )\n",
    "    \n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_with_openai(query: str, api_key: str):\n",
    "    \"\"\"\n",
    "    Generate text using the OpenAI provider via the WebSocket endpoint.\n",
    "\n",
    "    Parameters:\n",
    "    - query: The input text to generate from; the prompt.\n",
    "    - api_key: Your OpenAI API key.\n",
    "\n",
    "    Returns:\n",
    "    - A generator that streams chunks of generated text.\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/llm-dispatch/generate\"\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "    }\n",
    "    args = {\n",
    "        \"query\": query,\n",
    "        \"key\": api_key,\n",
    "        \"provider\": \"openai\",\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=args, stream=True)\n",
    "    if response.status_code != 200:\n",
    "        print('Network response was not ok')\n",
    "        print(response)\n",
    "        return\n",
    "\n",
    "    # Streaming response using the \"iter_content\" method\n",
    "    for chunk in response.iter_content():\n",
    "        if chunk:\n",
    "            yield chunk.decode('utf-8', errors='replace') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_classifier_predict(model_id: str, query: str, token: str, top_k=1):\n",
    "    \"\"\"\n",
    "    Predicts the NER tags for a given query.\n",
    "\n",
    "    Parameters:\n",
    "    - model_id: model ID as returned by create_token_classifier. You can also find the model ID in the list returned by list_models.\n",
    "    - query: The passage to predict the NER tags for.\n",
    "    - token: Authorization token from login\n",
    "    - top_k: The number of tags predicted for each token.\n",
    "\n",
    "    Returns a dictionary in this format:\n",
    "    {\n",
    "        \"text\": \"The text that was passed in\",\n",
    "        \"predicted_tags\": [\n",
    "            [\"TOP_TAG_FOR_FIRST_TOKEN\", \"SCORE_FOR_TOP_TAG_FOR_FIRST_TOKEN\", ...],\n",
    "            [\"TOP_TAG_FOR_SECOND_TOKEN\", \"SCORE_FOR_TOP_TAG_FOR_SECOND_TOKEN\", ...],\n",
    "            ...\n",
    "            [\"TOP_TAG_FOR_LAST_TOKEN\", \"SCORE_FOR_TOP_TAG_FOR_LAST_TOKEN\", ...]\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    The number of tags predicted for each token is specified in the top_k parameter.\n",
    "    \"\"\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    query_url = f\"{BASE_URL}/{model_id}/predict\"\n",
    "\n",
    "    base_params = {\"query\": query, \"top_k\": top_k}\n",
    "\n",
    "    response = requests.post(\n",
    "        query_url,\n",
    "        json=base_params,\n",
    "        headers=headers,\n",
    "    )\n",
    "\n",
    "    # Check if the query was successful; if not, raise an exception\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Query failed: {response.status_code}, {response.text}\")\n",
    "\n",
    "    return response.json()[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obfuscate_pii(token_classifier_model_id: str, text_chunks: List[str], auth_token: str):\n",
    "    \"\"\"\n",
    "    Obfuscates PII in the references using the NER model by replacing them with placeholders.\n",
    "\n",
    "    Parameters:\n",
    "    - token_classifier_model_id: model ID as returned by create_token_classifier. You can also find the model ID in the list returned by list_models.\n",
    "    - text_chunks: A list of strings containing the text to obfuscate.\n",
    "    - auth_token: Authorization token from login\n",
    "\n",
    "    Returns a tuple containing:\n",
    "    - A list of strings containing the obfuscated PII information.\n",
    "    - A dictionary containing the mapping of obfuscated tokens to original tokens.\n",
    "    \"\"\"\n",
    "    token_to_tag = {}\n",
    "    token_counts = {}\n",
    "\n",
    "    for text in text_chunks:\n",
    "        text = \" \".join(text.split())\n",
    "        predicted_tags = token_classifier_predict(auth_token, token_classifier_model_id, text)\n",
    "        predicted_tags = predicted_tags[\"predicted_tags\"]\n",
    "        for i, token in enumerate(text.split()):\n",
    "            tag = predicted_tags[i][0]\n",
    "            if tag != \"O\":\n",
    "                if token not in token_to_tag:\n",
    "                    tg = f\"<{tag}>\"\n",
    "                    token_to_tag[token] = tg\n",
    "    token_counts = {v: 0 for k, v in token_to_tag.items()}\n",
    "    inverse_map = {}\n",
    "\n",
    "    for k, v in token_to_tag.items():\n",
    "        new_tag = v[:-1] + f\"_{token_counts[v]}>\"\n",
    "        inverse_map[new_tag] = k\n",
    "        token_to_tag[k] = new_tag\n",
    "        token_counts[v] += 1\n",
    "\n",
    "    output_text = []\n",
    "    for text in text_chunks:\n",
    "        text = \" \".join(text.split())\n",
    "        redacted_text = [\n",
    "            word if word not in token_to_tag else token_to_tag[word]\n",
    "            for word in text.split()\n",
    "        ]\n",
    "        output_text.append(\" \".join(redacted_text))\n",
    "\n",
    "    return output_text, inverse_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_pii(text: str, tag_to_token: Dict[str, str]):\n",
    "    \"\"\"\n",
    "    Restores the PII in the text by replacing the placeholders with the original tokens.\n",
    "\n",
    "    Parameters:\n",
    "    - text: A string containing the obfuscated PII information.\n",
    "    - tag_to_token: A dictionary containing the mapping of obfuscated tokens to original tokens.\n",
    "\n",
    "    Returns a string containing the restored PII information.\n",
    "    \"\"\"\n",
    "    restored_text = []\n",
    "    for word in text.split():\n",
    "        word = strip_non_alphanumeric(word)\n",
    "        if word in tag_to_token.keys():\n",
    "            restored_text.append(tag_to_token[word])\n",
    "        else:\n",
    "            restored_text.append(word)\n",
    "    return \" \".join(restored_text)\n",
    "\n",
    "\n",
    "def strip_non_alphanumeric(word):\n",
    "    pattern = r\"^[^a-zA-Z0-9_<>\\s]+|[^a-zA-Z0-9_<>\\s]+$\"\n",
    "    cleaned_string = re.sub(pattern, \"\", word)\n",
    "    return cleaned_string\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
